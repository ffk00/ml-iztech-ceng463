{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYs1yOURJgTv"
      },
      "source": [
        "# CENG 463 HW 1– Water Resource Risk Classification\n",
        "**Start Date:**  \n",
        "**Due Date:** Month Dayth, 2026\n",
        "\n",
        "## Dataset Overview\n",
        "The dataset utilized in this assignment originates from the World Resources Institute (WRI) – Aqueduct Water Risk Atlas. It provides country-level indicators describing key hydrological and environmental factors, which are listed in the table below. The objective is to classify each country into a Water Resource Risk Category (0-4) using these indicators. Students are also expected to create two derived features — Composite Water Stress Index (CWSI) and Seasonal–Flood Interaction (SFI).\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| gid_0 | Country Code |\n",
        "| bws_score | Baseline Water Stress |\n",
        "| gtd_score | Groundwater Depletion |\n",
        "| drr_score | Drought Risk |\n",
        "| rfr_score | River Flood Risk |\n",
        "| sev_score | Seasonal Variability |\n",
        "| w_awr_def_tot_cat | Target: Water Risk Category (0-4) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJJc9KioJgWI"
      },
      "source": [
        "## 1. Feature Engineering (35 pts)\n",
        "Students are expected to create two new features based on the existing indicators:\n",
        "\n",
        "1. **Composite Water Stress Index (CWSI):**\n",
        "   CWSI combines baseline water stress, groundwater depletion, and drought risk.\n",
        "   Formula: CWSI = 0.5 × bws_score + 0.3 × gtd_score + 0.2 × drr_score\n",
        "\n",
        "2. **Seasonal–Flood Interaction (SFI):**\n",
        "   SFI represents interaction between seasonal variability and river flood risk.\n",
        "   Formula: SFI = sev_score × rfr_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create CWSI and SFI features\n",
        "# and country column has processed\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"water_risk_dataset.csv\")\n",
        "\n",
        "df['CWSI'] =  0.5 * df['bws_score'] + 0.3 * df['gtd_score'] + 0.2 * df['drr_score']\n",
        "df['SFI'] = df['sev_score'] * df['rfr_score']\n",
        "\n",
        "target_col = 'w_awr_def_tot_cat'\n",
        "X=df.drop(columns=[target_col])\n",
        "y=df[target_col]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "uDnInMIuKCeI"
      },
      "outputs": [],
      "source": [
        "cat_cols = ['gid_0']\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "\t('encoder', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
        "\t('scaler', StandardScaler(), num_cols)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auuRhydMJgYg"
      },
      "source": [
        "## 2. Model Training & Evaluation (40 pts)\n",
        "Train five classification models: Random Forest, SVM, KNN, Gaussian Naive Bayes, Logistic Regression.\n",
        "\n",
        "*Hint: Use scaled data for SVM, KNN, Logistic Regression.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Split data into features X and target y\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_X,test_X,train_y,test_y=train_test_split(X,y,test_size=0.25,random_state=42)\n",
        "print(\"data splitted into train and test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN pipeline trained.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_pipe = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "knn_pipe.fit(train_X, train_y)\n",
        "print(\"KNN pipeline trained.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.917\n",
            "Test Accuracy:  0.869\n",
            "\n",
            "Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.97      0.96       186\n",
            "         1.0       0.82      0.86      0.84       208\n",
            "         2.0       0.81      0.75      0.78       216\n",
            "         3.0       0.83      0.85      0.84       240\n",
            "         4.0       0.92      0.92      0.92       289\n",
            "\n",
            "    accuracy                           0.87      1139\n",
            "   macro avg       0.87      0.87      0.87      1139\n",
            "weighted avg       0.87      0.87      0.87      1139\n",
            "\n",
            "Confusion Matrix (Test):\n",
            "[[180   6   0   0   0]\n",
            " [  6 179  22   1   0]\n",
            " [  2  31 162  21   0]\n",
            " [  0   1  14 203  22]\n",
            " [  0   0   3  20 266]]\n"
          ]
        }
      ],
      "source": [
        "# KNN evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Predictions\n",
        "y_pred_train = knn_pipe.predict(train_X)\n",
        "y_pred_test = knn_pipe.predict(test_X)\n",
        "\n",
        "# Accuracy\n",
        "train_acc = accuracy_score(train_y, y_pred_train)\n",
        "test_acc = accuracy_score(test_y, y_pred_test)\n",
        "print(f\"Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"Test Accuracy:  {test_acc:.3f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(test_y, y_pred_test))\n",
        "\n",
        "print(\"Confusion Matrix (Test):\")\n",
        "print(confusion_matrix(test_y, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIEFe653KGE4"
      },
      "outputs": [],
      "source": [
        "# TODO: Train models and evaluate accuracy\n",
        "# TODO: Prepare evaluation table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlCs3MIXJgdI"
      },
      "source": [
        "## 3. Hyperparameter Optimization (20 pts)\n",
        "Tune each model using GridSearchCV with 5-fold CV.\n",
        "Compare baseline and tuned results and report improvements.\n",
        "Identify the model with highest tuned performance.\n",
        "\n",
        "Hint: Use accuracy as scoring metric. Add classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcGBEmL7KN7Q"
      },
      "outputs": [],
      "source": [
        "# TODO: Define parameter grids for each model\n",
        "# TODO: Perform GridSearchCV and compare results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPdWaMf3Jghw"
      },
      "source": [
        "## 4. Feature Importance Analysis (5 pts)\n",
        "Choose one model and analyze feature importance. Present most influential features in a table and bar chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLRwzRy9KjJg"
      },
      "outputs": [],
      "source": [
        "# TODO: Select your best model (e.g., Random Forest)\n",
        "# TODO: Train the model on the training data if not already trained\n",
        "# TODO: Calculate feature importances\n",
        "# TODO: Create a table of features sorted by importance\n",
        "# TODO: Plot a bar chart of feature importances\n",
        "# TODO: Optional: Comment on top 3-5 most influential features"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
